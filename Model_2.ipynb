{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/e.x.coco/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------\n",
    "# üìÇ Importations n√©cessaires\n",
    "# ------------------------------------------\n",
    "\n",
    "# Biblioth√®ques standard Python\n",
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Biblioth√®ques de manipulation de donn√©es\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump\n",
    "\n",
    "# Biblioth√®ques de visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    ConfusionMatrixDisplay,\n",
    "    recall_score,\n",
    "    roc_curve,\n",
    "    auc\n",
    ")\n",
    "\n",
    "# TensorFlow et Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ReduceLROnPlateau,\n",
    "    Callback\n",
    ")\n",
    "from tensorflow.keras.layers import (\n",
    "    GRU,\n",
    "    Dense,\n",
    "    Input,\n",
    "    Dropout,\n",
    "    LayerNormalization,\n",
    "    Flatten,\n",
    "    Attention,\n",
    "    MultiHeadAttention,\n",
    "    GlobalAveragePooling1D,\n",
    "    Lambda\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# üöÄ Chargement des donn√©es locales\n",
    "# ------------------------------------------\n",
    "\n",
    "# Fichiers d'entr√©e\n",
    "train_file = \"../Cleaned_data/mitbih_train_trimmed.csv\"\n",
    "test_file = \"../Cleaned_data/mitbih_test_trimmed.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des donn√©es depuis mitbih_train_trimmed.csv...\n",
      "Nombre d'√©chantillons : 87553\n",
      "Distribution des classes : Counter({0: 72470, 4: 6431, 2: 5788, 1: 2223, 3: 641})\n",
      "Chargement des donn√©es depuis mitbih_test_trimmed.csv...\n",
      "Nombre d'√©chantillons : 21891\n",
      "Distribution des classes : Counter({0: 18117, 4: 1608, 2: 1448, 1: 556, 3: 162})\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------\n",
    "# üì• Chargement des donn√©es avec fonction\n",
    "# ------------------------------------------\n",
    "\n",
    "def load_data(file_path):\n",
    "    print(f\"Chargement des donn√©es depuis {file_path}...\")\n",
    "    df = pd.read_csv(file_path)  # Chargement du fichier CSV\n",
    "    X = df.iloc[:, :-1].values  # Toutes les colonnes sauf la derni√®re (features)\n",
    "    y = df.iloc[:, -1].astype(int).values  # Derni√®re colonne = labels\n",
    "    X, y = shuffle(X, y, random_state=42)  # M√©lange al√©atoire des donn√©es\n",
    "    print(f\"Nombre d'√©chantillons : {len(X)}\")\n",
    "    print(\"Distribution des classes :\", Counter(y))  # Distribution des classes\n",
    "    return X, y\n",
    "\n",
    "# Charger les donn√©es en utilisant la fonction\n",
    "train_ecgs, train_labels = load_data(train_file)\n",
    "test_ecgs, test_labels = load_data(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------\n",
    "# üîÑ Augmentation des donn√©es\n",
    "# ------------------------------------------\n",
    "\n",
    "# Fonctions d'augmentation\n",
    "def add_gaussian_noise(ecg, noise_level=0.01):\n",
    "   noise = np.random.normal(0, noise_level, len(ecg))\n",
    "   return ecg + noise\n",
    "\n",
    "def shift_signal(ecg, shift=10):\n",
    "   return np.roll(ecg, shift)\n",
    "\n",
    "def combine_augmentations(ecg, noise_level=0.01, shift=10):\n",
    "   return shift_signal(add_gaussian_noise(ecg, noise_level), shift)\n",
    "\n",
    "def augment_class_diverse(ecgs, deficit):\n",
    "   augmented_ecgs = []\n",
    "   num_per_type = deficit // 3\n",
    "   remaining = deficit % 3\n",
    "\n",
    "   # Donn√©es d√©phas√©es\n",
    "   for i in range(num_per_type):\n",
    "       ecg = ecgs[i % len(ecgs)]\n",
    "       augmented_ecgs.append(shift_signal(ecg))\n",
    "\n",
    "   # Donn√©es bruit√©es\n",
    "   for i in range(num_per_type):\n",
    "       ecg = ecgs[i % len(ecgs)]\n",
    "       augmented_ecgs.append(add_gaussian_noise(ecg))\n",
    "\n",
    "   # Donn√©es combin√©es\n",
    "   for i in range(num_per_type + remaining):\n",
    "       ecg = ecgs[i % len(ecgs)]\n",
    "       augmented_ecgs.append(combine_augmentations(ecg))\n",
    "\n",
    "   return augmented_ecgs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------\n",
    "# ‚ûó S√©parer les donn√©es par classes\n",
    "# ------------------------------------------\n",
    "\n",
    "\n",
    "def separate_by_class(ecgs, labels):\n",
    "   classes = {i: [] for i in range(5)}\n",
    "   for ecg, label in zip(ecgs, labels):\n",
    "       classes[label].append(ecg)\n",
    "   return classes\n",
    "\n",
    "# Charger les donn√©es\n",
    "train_file = \"mitbih_train_trimmed.csv\"\n",
    "test_file = \"mitbih_test_trimmed.csv\"\n",
    "\n",
    "train_ecgs, train_labels = load_data(train_file)\n",
    "test_ecgs, test_labels = load_data(test_file)\n",
    "\n",
    "\n",
    "# S√©parer par classe\n",
    "train_classes = separate_by_class(train_ecgs, train_labels)\n",
    "test_classes = separate_by_class(test_ecgs, test_labels)\n",
    "\n",
    "# Trouver la taille cible (max des classes malades)\n",
    "target_size = max(len(train_classes[i]) for i in range(1, 5))\n",
    "print(f\"\\nTaille cible pour chaque classe malade : {target_size}\")\n",
    "\n",
    "# Augmenter chaque classe malade\n",
    "balanced_malades_ecgs = []\n",
    "balanced_malades_labels = []\n",
    "\n",
    "for i in range(1, 5):\n",
    "   deficit = target_size - len(train_classes[i])\n",
    "   if deficit > 0:\n",
    "       augmented = augment_class_diverse(train_classes[i], deficit)\n",
    "       train_classes[i].extend(augmented)\n",
    "\n",
    "   balanced_malades_ecgs.extend(train_classes[i])\n",
    "   balanced_malades_labels.extend([i] * len(train_classes[i]))\n",
    "   print(f\"Classe {i}: {len(train_classes[i])} √©chantillons\")\n",
    "\n",
    "# M√©langer les donn√©es\n",
    "balanced_malades_ecgs, balanced_malades_labels = shuffle(balanced_malades_ecgs, balanced_malades_labels)\n",
    "\n",
    "# Pr√©paration des donn√©es de test (malades uniquement)\n",
    "test_malades_ecgs = []\n",
    "test_malades_labels = []\n",
    "for i in range(1, 5):\n",
    "   test_malades_ecgs.extend(test_classes[i])\n",
    "   test_malades_labels.extend([i] * len(test_classes[i]))\n",
    "\n",
    "\n",
    "# Visualisation des distributions finales\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Distribution train\n",
    "plt.subplot(1, 2, 1)\n",
    "train_dist = Counter(balanced_malades_labels)\n",
    "plt.pie(\n",
    "   [train_dist[i] for i in range(1, 5)],\n",
    "   labels=[f\"Classe {i}\" for i in range(1, 5)],\n",
    "   autopct='%1.1f%%',\n",
    "   startangle=90,\n",
    "   colors=['#B8D8E8', '#C7E5D6', '#D5C8E6', '#E8D8E8']\n",
    ")\n",
    "plt.title(\"Distribution des classes malades (Train)\")\n",
    "\n",
    "# Distribution test\n",
    "plt.subplot(1, 2, 2)\n",
    "test_dist = Counter(test_malades_labels)\n",
    "plt.pie(\n",
    "   [test_dist[i] for i in range(1, 5)],\n",
    "   labels=[f\"Classe {i}\" for i in range(1, 5)],\n",
    "   autopct='%1.1f%%',\n",
    "   startangle=90,\n",
    "   colors=['#B8D8E8', '#C7E5D6', '#D5C8E6', '#E8D8E8']\n",
    ")\n",
    "plt.title(\"Distribution des classes malades (Test)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation des donn√©es\n",
    "scaler = StandardScaler()\n",
    "normalized_train_ecgs = scaler.fit_transform(balanced_malades_ecgs)\n",
    "normalized_test_ecgs = scaler.transform(test_malades_ecgs)\n",
    "\n",
    "# Reshape pour GRU\n",
    "normalized_train_ecgs = normalized_train_ecgs.reshape(normalized_train_ecgs.shape[0], normalized_train_ecgs.shape[1], 1)\n",
    "normalized_test_ecgs = normalized_test_ecgs.reshape(normalized_test_ecgs.shape[0], normalized_test_ecgs.shape[1], 1)\n",
    "\n",
    "# Convertir les labels en arrays NumPy\n",
    "balanced_malades_labels = np.array(balanced_malades_labels)\n",
    "test_malades_labels = np.array(test_malades_labels)\n",
    "\n",
    "print(\"Shapes des donn√©es :\")\n",
    "print(\"Train:\", normalized_train_ecgs.shape)\n",
    "print(\"Test:\", normalized_test_ecgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback pour suivre le recall\n",
    "class RecallCallback(Callback):\n",
    "    def __init__(self, validation_data=None, training_data=None):\n",
    "        super(RecallCallback, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.training_data = training_data\n",
    "        self.train_recalls = []\n",
    "        self.val_recalls = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # Calcul du recall sur les donn√©es d'entra√Ænement\n",
    "        y_pred = self.model.predict(self.training_data[0])\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "        train_recall = recall_score(self.training_data[1], y_pred_classes, average='macro')\n",
    "        self.train_recalls.append(train_recall)\n",
    "        logs['recall'] = train_recall\n",
    "\n",
    "        # Calcul du recall sur les donn√©es de validation\n",
    "        if self.validation_data:\n",
    "            val_pred = self.model.predict(self.validation_data[0])\n",
    "            val_pred_classes = np.argmax(val_pred, axis=1)\n",
    "            val_recall = recall_score(self.validation_data[1], val_pred_classes, average='macro')\n",
    "            self.val_recalls.append(val_recall)\n",
    "            logs['val_recall'] = val_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_auto_balanced_thresholds(y_true_bin, y_pred_proba, min_recall=0.95, n_thresholds=1000):\n",
    "    \"\"\"\n",
    "    Trouve automatiquement les seuils optimaux en √©quilibrant recall et pr√©cision.\n",
    "    \"\"\"\n",
    "    n_classes = y_true_bin.shape[1]\n",
    "    optimal_thresholds = []\n",
    "    final_metrics = []\n",
    "\n",
    "    for classe in range(n_classes):\n",
    "        print(f\"\\nOptimisation pour la classe {classe + 1}:\")\n",
    "\n",
    "        # G√©n√©rer les seuils √† tester\n",
    "        probs_class = y_pred_proba[:, classe]\n",
    "        min_prob, max_prob = np.min(probs_class), np.max(probs_class)\n",
    "        thresholds = np.linspace(min_prob, max_prob, n_thresholds)\n",
    "\n",
    "        # Calculer les m√©triques pour tous les seuils\n",
    "        all_metrics = []\n",
    "        for threshold in thresholds:\n",
    "            y_pred_bin = (probs_class >= threshold).astype(int)\n",
    "\n",
    "            true_pos = np.sum((y_true_bin[:, classe] == 1) & (y_pred_bin == 1))\n",
    "            false_pos = np.sum((y_true_bin[:, classe] == 0) & (y_pred_bin == 1))\n",
    "            false_neg = np.sum((y_true_bin[:, classe] == 1) & (y_pred_bin == 0))\n",
    "\n",
    "            recall = true_pos / (true_pos + false_neg + 1e-10)\n",
    "            precision = true_pos / (true_pos + false_pos + 1e-10)\n",
    "            f1 = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "\n",
    "            all_metrics.append({\n",
    "                'threshold': threshold,\n",
    "                'recall': recall,\n",
    "                'precision': precision,\n",
    "                'f1': f1\n",
    "            })\n",
    "\n",
    "        # Trouver les points d'√©quilibre potentiels\n",
    "        valid_metrics = []\n",
    "        for min_precision_test in np.linspace(0.1, 0.9, 50):\n",
    "            # Filtrer les seuils qui donnent un recall suffisant\n",
    "            recall_valid = [m for m in all_metrics if m['recall'] >= min_recall]\n",
    "            if not recall_valid:\n",
    "                continue\n",
    "\n",
    "            # Parmi ceux-l√†, chercher ceux avec une pr√©cision suffisante\n",
    "            precision_valid = [m for m in recall_valid if m['precision'] >= min_precision_test]\n",
    "            if not precision_valid:\n",
    "                continue\n",
    "\n",
    "            # Calculer un score d'√©quilibre\n",
    "            best_for_threshold = max(precision_valid, key=lambda x: x['f1'])\n",
    "            balance_score = best_for_threshold['recall'] * best_for_threshold['precision']\n",
    "\n",
    "            valid_metrics.append({\n",
    "                'min_precision': min_precision_test,\n",
    "                'metrics': best_for_threshold,\n",
    "                'balance_score': balance_score\n",
    "            })\n",
    "\n",
    "        if valid_metrics:\n",
    "            # Choisir le meilleur point d'√©quilibre\n",
    "            best_balance = max(valid_metrics, key=lambda x: x['balance_score'])\n",
    "            best_metric = best_balance['metrics']\n",
    "            used_min_precision = best_balance['min_precision']\n",
    "        else:\n",
    "            # Si aucun point d'√©quilibre n'est trouv√©, prendre le meilleur compromis\n",
    "            print(f\"Attention: Impossible de trouver un √©quilibre optimal pour la classe {classe + 1}\")\n",
    "            best_metric = max(all_metrics, key=lambda x: x['recall'] * x['precision'])\n",
    "            used_min_precision = best_metric['precision']\n",
    "\n",
    "        optimal_thresholds.append(best_metric['threshold'])\n",
    "        final_metrics.append(best_metric)\n",
    "\n",
    "        print(f\"Classe {classe + 1}:\")\n",
    "        print(f\"- Seuil optimal trouv√©: {best_metric['threshold']:.4f}\")\n",
    "        print(f\"- Pr√©cision minimum utilis√©e: {used_min_precision:.4f}\")\n",
    "        print(f\"- Recall obtenu: {best_metric['recall']:.4f}\")\n",
    "        print(f\"- Pr√©cision obtenue: {best_metric['precision']:.4f}\")\n",
    "        print(f\"- F1-score: {best_metric['f1']:.4f}\")\n",
    "\n",
    "        # Visualiser la courbe precision-recall pour cette classe\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Tracer la courbe precision-recall\n",
    "        precisions = [m['precision'] for m in all_metrics]\n",
    "        recalls = [m['recall'] for m in all_metrics]\n",
    "        plt.plot(recalls, precisions, 'b-', label='Precision-Recall curve')\n",
    "\n",
    "        # Marquer le point optimal\n",
    "        plt.plot(best_metric['recall'], best_metric['precision'], 'ro',\n",
    "                markersize=10, label='Point optimal')\n",
    "\n",
    "        # Lignes de r√©f√©rence\n",
    "        plt.axvline(x=min_recall, color='g', linestyle='--',\n",
    "                   label=f'Recall minimum ({min_recall})')\n",
    "        plt.axhline(y=used_min_precision, color='r', linestyle='--',\n",
    "                   label=f'Pr√©cision minimum ({used_min_precision:.2f})')\n",
    "\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Pr√©cision')\n",
    "        plt.title(f'Courbe Precision-Recall - Classe {classe + 1}')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return optimal_thresholds, final_metrics\n",
    "\n",
    "def predict_with_optimal_thresholds(probas, thresholds):\n",
    "    \"\"\"\n",
    "    Applique les seuils optimaux aux probabilit√©s pr√©dites.\n",
    "    \"\"\"\n",
    "    n_samples = len(probas)\n",
    "    predictions = np.zeros(n_samples, dtype=int)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        # Comparer chaque probabilit√© avec son seuil\n",
    "        above_threshold = probas[i] >= thresholds\n",
    "\n",
    "        if np.any(above_threshold):\n",
    "            # Si au moins une classe d√©passe son seuil,\n",
    "            # choisir celle avec la plus grande marge au-dessus de son seuil\n",
    "            margins = probas[i] / np.array(thresholds)\n",
    "            predictions[i] = np.argmax(margins)\n",
    "        else:\n",
    "            # Si aucune classe ne d√©passe son seuil,\n",
    "            # choisir celle avec la plus grande probabilit√© relative\n",
    "            predictions[i] = np.argmax(probas[i])\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Param√®tres du mod√®le\n",
    "input_shape = normalized_train_ecgs.shape[1:]\n",
    "gru_units = 64\n",
    "dropout_rate = 0.1\n",
    "\n",
    "def gru_attention_block(inputs, gru_units, dropout_rate):\n",
    "    x = GRU(gru_units, return_sequences=True)(inputs)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = GRU(gru_units * 2, return_sequences=True)(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    attention_output = Attention()([x, x])\n",
    "    return attention_output\n",
    "\n",
    "# Construction du mod√®le\n",
    "inputs = Input(shape=input_shape)\n",
    "x = gru_attention_block(inputs, gru_units, dropout_rate)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = LayerNormalization()(x)\n",
    "x = Dropout(dropout_rate)(x)\n",
    "outputs = Dense(4, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©paration des donn√©es\n",
    "val_split = 0.2\n",
    "split_idx = int(normalized_train_ecgs.shape[0] * (1 - val_split))\n",
    "X_train = normalized_train_ecgs[:split_idx]\n",
    "X_val = normalized_train_ecgs[split_idx:]\n",
    "y_train = balanced_malades_labels[:split_idx] - 1\n",
    "y_val = balanced_malades_labels[split_idx:] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du callback de recall\n",
    "recall_callback = RecallCallback(\n",
    "    validation_data=(X_val, y_val),\n",
    "    training_data=(X_train, y_train)\n",
    ")\n",
    "\n",
    "# Callbacks ajust√©s\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=10,  # Garder √† 10\n",
    "        restore_best_weights=True,\n",
    "        mode='min'  # Explicitement sp√©cifier le mode\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.3,  # Chang√© de 0.5 √† 0.3 pour une r√©duction plus progressive\n",
    "        patience=6,  # Augment√© de 5 √† 6\n",
    "        min_lr=1e-6,\n",
    "        mode='min',\n",
    "        cooldown=2  # Ajouter un cooldown pour √©viter des changements trop fr√©quents\n",
    "    ),\n",
    "    recall_callback\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Ænement\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    "    shuffle=True  # Assurer un bon m√©lange des donn√©es √† chaque √©poque\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le scaler\n",
    "dump(scaler, 'ecg_scaler_m2.joblib')\n",
    "print(\"Scaler sauvegard√© sous 'ecg_scaler_m2.joblib'.\")\n",
    "\n",
    "# Sauvegarder le mod√®le\n",
    "model.save('ecg_model_m2.h5')\n",
    "print(\"Mod√®le sauvegard√© sous 'ecg_model_m2.h5'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des m√©triques d'entra√Ænement\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Courbe de perte\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('√âvolution de la fonction de perte')\n",
    "plt.xlabel('√âpoque')\n",
    "plt.ylabel('Perte')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Courbe d'accuracy\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('√âvolution de l\\'accuracy')\n",
    "plt.xlabel('√âpoque')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Courbe de recall\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(recall_callback.train_recalls, label='Training Recall')\n",
    "plt.plot(recall_callback.val_recalls, label='Validation Recall')\n",
    "plt.title('√âvolution du Recall')\n",
    "plt.xlabel('√âpoque')\n",
    "plt.ylabel('Recall')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Affichage des r√©sultats pr√©cis\n",
    "print(\"\\n--- R√©sultats des m√©triques d'entra√Ænement et de validation ---\")\n",
    "print(f\"Perte finale sur l'ensemble d'entra√Ænement : {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Perte finale sur l'ensemble de validation : {history.history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Meilleure perte (validation) : {min(history.history['val_loss']):.4f}\")\n",
    "\n",
    "print(f\"\\nAccuracy finale sur l'ensemble d'entra√Ænement : {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Accuracy finale sur l'ensemble de validation : {history.history['val_accuracy'][-1]:.4f}\")\n",
    "print(f\"Meilleure accuracy (validation) : {max(history.history['val_accuracy']):.4f}\")\n",
    "\n",
    "if recall_callback.train_recalls and recall_callback.val_recalls:\n",
    "    print(f\"\\nRecall final sur l'ensemble d'entra√Ænement : {recall_callback.train_recalls[-1]:.4f}\")\n",
    "    print(f\"Recall final sur l'ensemble de validation : {recall_callback.val_recalls[-1]:.4f}\")\n",
    "    print(f\"Meilleur recall (validation) : {max(recall_callback.val_recalls):.4f}\")\n",
    "else:\n",
    "    print(\"\\nAucune valeur de Recall disponible.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === R√©sultats avec le seuil de base (0.5) ===\n",
    "print(\"=== R√©sultats avec le seuil de base (0.5) ===\")\n",
    "\n",
    "# Courbes ROC pour le seuil de base\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['blue', 'red', 'green', 'purple']\n",
    "\n",
    "# Tracer les courbes ROC pour chaque classe avec seuil de base\n",
    "for i in range(4):\n",
    "    fpr, tpr, _ = roc_curve(test_labels_bin[:, i], y_pred_proba[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, color=colors[i], lw=2,\n",
    "             label=f'Classe {i+1} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taux de faux positifs')\n",
    "plt.ylabel('Taux de vrais positifs')\n",
    "plt.title('Courbes ROC - Seuil de base (0.5)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Calcul des pr√©dictions avec le seuil de base (0.5)\n",
    "y_pred_classes_base = (y_pred_proba >= 0.5).astype(int).argmax(axis=1) + 1  # +1 pour les classes 1-4\n",
    "\n",
    "# Matrices de confusion pour le seuil de base\n",
    "cm_base = confusion_matrix(test_malades_labels, y_pred_classes_base)\n",
    "cm_percent_base = (cm_base.astype('float') / cm_base.sum(axis=1)[:, np.newaxis] * 100)\n",
    "\n",
    "# Matrice de confusion en effectifs\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_base, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Classe 1', 'Classe 2', 'Classe 3', 'Classe 4'],\n",
    "            yticklabels=['Classe 1', 'Classe 2', 'Classe 3', 'Classe 4'])\n",
    "plt.xlabel('Pr√©dictions')\n",
    "plt.ylabel('Vraies classes')\n",
    "plt.title('Matrice de Confusion (Effectifs) - Seuil de base (0.5)')\n",
    "plt.show()\n",
    "\n",
    "# Matrice de confusion en pourcentages\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_percent_base, annot=True, fmt='.1f', cmap='Blues',\n",
    "            xticklabels=['Classe 1', 'Classe 2', 'Classe 3', 'Classe 4'],\n",
    "            yticklabels=['Classe 1', 'Classe 2', 'Classe 3', 'Classe 4'])\n",
    "plt.xlabel('Pr√©dictions')\n",
    "plt.ylabel('Vraies classes')\n",
    "plt.title('Matrice de Confusion (%) - Seuil de base (0.5)')\n",
    "plt.show()\n",
    "\n",
    "# Calcul des m√©triques d√©taill√©es\n",
    "metrics_base = []\n",
    "for i in range(4):\n",
    "    true_pos = cm_base[i, i]\n",
    "    false_neg = np.sum(cm_base[i, :]) - true_pos\n",
    "    false_pos = np.sum(cm_base[:, i]) - true_pos\n",
    "    true_neg = np.sum(cm_base) - true_pos - false_pos - false_neg\n",
    "\n",
    "    precision = true_pos / (true_pos + false_pos) if (true_pos + false_pos) != 0 else 0\n",
    "    recall = true_pos / (true_pos + false_neg) if (true_pos + false_neg) != 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    support = true_pos + false_neg\n",
    "\n",
    "    metrics_base.append({\n",
    "        \"Classe\": f\"Classe {i+1}\",\n",
    "        \"Pr√©cision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-score\": f1,\n",
    "        \"Support\": support\n",
    "    })\n",
    "\n",
    "# Rapport de classification pour le seuil de base\n",
    "print(\"\\nRapport de classification complet pour le seuil de base (0.5):\")\n",
    "print(classification_report(test_malades_labels, y_pred_classes_base, digits=4))\n",
    "\n",
    "# Affichage des m√©triques sous forme de tableau\n",
    "metrics_base_df = pd.DataFrame(metrics_base)\n",
    "metrics_base_df = metrics_base_df.set_index(\"Classe\")\n",
    "print(\"\\nTableau r√©capitulatif des m√©triques pour le seuil de base:\")\n",
    "print(metrics_base_df)\n",
    "\n",
    "# Visualisation des m√©triques sous forme de tableau\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "table_data = table(ax, metrics_base_df, loc='center', colWidths=[0.2]*len(metrics_base_df.columns))\n",
    "table_data.auto_set_font_size(False)\n",
    "table_data.set_fontsize(10)\n",
    "table_data.scale(1.2, 1.2)\n",
    "plt.title(\"Tableau r√©capitulatif des m√©triques - Seuil de base (0.5)\", fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === R√©sultats avec les seuils optimaux ===\n",
    "print(\"=== R√©sultats avec les seuils optimaux ===\")\n",
    "\n",
    "# Courbes ROC avec seuils optimaux\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['blue', 'red', 'green', 'purple']\n",
    "\n",
    "# Tracer les courbes ROC pour chaque classe avec seuils optimaux\n",
    "for i in range(4):\n",
    "    fpr, tpr, thresholds = roc_curve(test_labels_bin[:, i], y_pred_proba[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Trouver les coordonn√©es du seuil optimal\n",
    "    optimal_idx = np.argmin(np.abs(thresholds - optimal_thresholds[i]))\n",
    "    optimal_fpr = fpr[optimal_idx]\n",
    "    optimal_tpr = tpr[optimal_idx]\n",
    "\n",
    "    # Tracer la courbe ROC\n",
    "    plt.plot(fpr, tpr, color=colors[i], lw=2,\n",
    "             label=f'Classe {i+1} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "    # Ajouter un point pour le seuil optimal\n",
    "    plt.scatter(optimal_fpr, optimal_tpr, color=colors[i], s=100, edgecolors='black',\n",
    "                label=f'Seuil optimal Classe {i+1} ({optimal_thresholds[i]:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taux de faux positifs')\n",
    "plt.ylabel('Taux de vrais positifs')\n",
    "plt.title('Courbes ROC avec seuils optimaux')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Application des seuils optimaux\n",
    "y_pred_classes_optimal = predict_with_optimal_thresholds(y_pred_proba, optimal_thresholds) + 1\n",
    "\n",
    "# Matrices de confusion pour les seuils optimaux\n",
    "cm_optimal = confusion_matrix(test_malades_labels, y_pred_classes_optimal)\n",
    "cm_percent_optimal = (cm_optimal.astype('float') / cm_optimal.sum(axis=1)[:, np.newaxis] * 100)\n",
    "\n",
    "# Matrice de confusion en effectifs\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_optimal, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Classe 1', 'Classe 2', 'Classe 3', 'Classe 4'],\n",
    "            yticklabels=['Classe 1', 'Classe 2', 'Classe 3', 'Classe 4'])\n",
    "plt.xlabel('Pr√©dictions')\n",
    "plt.ylabel('Vraies classes')\n",
    "plt.title('Matrice de Confusion (Effectifs) - Seuils optimaux')\n",
    "plt.show()\n",
    "\n",
    "# Matrice de confusion en pourcentages\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_percent_optimal, annot=True, fmt='.1f', cmap='Blues',\n",
    "            xticklabels=['Classe 1', 'Classe 2', 'Classe 3', 'Classe 4'],\n",
    "            yticklabels=['Classe 1', 'Classe 2', 'Classe 3', 'Classe 4'])\n",
    "plt.xlabel('Pr√©dictions')\n",
    "plt.ylabel('Vraies classes')\n",
    "plt.title('Matrice de Confusion (%) - Seuils optimaux')\n",
    "plt.show()\n",
    "\n",
    "# Affichage des m√©triques d√©taill√©es\n",
    "metrics_optimal = []\n",
    "for i in range(4):\n",
    "    true_pos = cm_optimal[i, i]\n",
    "    false_neg = np.sum(cm_optimal[i, :]) - true_pos\n",
    "    false_pos = np.sum(cm_optimal[:, i]) - true_pos\n",
    "    true_neg = np.sum(cm_optimal) - true_pos - false_pos - false_neg\n",
    "\n",
    "    precision = true_pos / (true_pos + false_pos) if (true_pos + false_pos) != 0 else 0\n",
    "    recall = true_pos / (true_pos + false_neg) if (true_pos + false_neg) != 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "    support = true_pos + false_neg\n",
    "\n",
    "    metrics_optimal.append({\n",
    "        \"Classe\": f\"Classe {i+1}\",\n",
    "        \"Seuil Optimal\": optimal_thresholds[i],\n",
    "        \"Pr√©cision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-score\": f1,\n",
    "        \"Support\": support\n",
    "    })\n",
    "\n",
    "# Rapport de classification pour les seuils optimaux\n",
    "print(\"\\nRapport de classification complet pour les seuils optimaux:\")\n",
    "print(classification_report(test_malades_labels, y_pred_classes_optimal, digits=4))\n",
    "\n",
    "# Affichage des m√©triques sous forme de tableau\n",
    "metrics_optimal_df = pd.DataFrame(metrics_optimal)\n",
    "metrics_optimal_df = metrics_optimal_df.set_index(\"Classe\")\n",
    "print(\"\\nTableau r√©capitulatif des m√©triques pour les seuils optimaux:\")\n",
    "print(metrics_optimal_df)\n",
    "\n",
    "# Visualisation des m√©triques sous forme de tableau\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "table_data = table(ax, metrics_optimal_df, loc='center', colWidths=[0.2]*len(metrics_optimal_df.columns))\n",
    "table_data.auto_set_font_size(False)\n",
    "table_data.set_fontsize(10)\n",
    "table_data.scale(1.2, 1.2)\n",
    "plt.title(\"Tableau r√©capitulatif des m√©triques - Seuils optimaux\", fontsize=14)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
